API-сервер
Сервер Kube API предоставляет доступ к Kubernetes REST API.
Центральный компонент Kubernetes, единственный кто общается с ETCD, работает по REST-API, через него проходит авторизация и аутентификация в кластере. 
Он не обладает состоянием и хранит все данные в кластере etcd, поэтому его несложно горизонтально масштабировать. 
API-сервер олицетворяет собой панель управления Kubernetes.
kube-apiserver — это внешний сервер для управляющего уровня, который обрабатывает API-запросы.

1) получение данных с сервера (обычно в формате JSON, или XML)  2)добавление новых данных на сервер  3)модификация существующих данных на сервере  4)удаление данных на сервере 
Операция получения данных не может приводить к изменению состояния сервера. Для каждого типа операции используется свой метод HTTP-запроса
1)получение - GET  2)добавление - POST  3)модификация - PUT  4)удаление - DELETE   (ниже указаны примеры)
GET-запрос /rest/users - получение информации о всех пользователях   GET-запрос /rest/users/125 - получение информации о пользователе с id=125 
POST-запрос /rest/users - добавление нового пользователя 
PUT-запрос /rest/users/125 - изменение информации о пользователе с id=125 
DELETE-запрос /rest/users/125 - удаление пользователя с id=125

Etcd — это высоконадежное распределенное хранилище данных. etcd — база данных, в которой Kubernetes хранит всю информацию о существующих узлах, ресурсах кластера и т. д.
Kubernetes хранит в нем все состояние своих кластеров. Требует быстрых дисков.
В небольших временных кластерах etcd можно запускать в единственном экземпляре и на одном узле с ведущими компонентами. 
В важных системах, требующих избыточности и высокой доступности, etcd обычно работает в виде кластера, состоящего из трех или даже пяти узлов.

Диспетчер контроллеров Kube представляет собой набор различных управляющих инструментов, упакованных в единый двоичный файл. Controller-manager отвечает за запуск контроллеров ресурсов
Он содержит контроллер репликации, pod-контроллер, контроллер сервисов, контроллер конечных точек, GarbageCollector - сборщик мусора и т.д. 
Все эти инструменты отслеживают работу кластера через API и при необходимости приводят его в нужное состояние.

Компонент kube-scheduler занимается планированием развертывания подов на узлах. kube-scheduler определяет, где будут запущены свежесозданные pod-оболочки.
Это крайне сложная задача, которая требует учета множества факторов, зависящих друг от друга, например: требований к ресурсам; ограничений и допусков; местонахождения данных;
требований к сервисам; политики аппаратных/программных ограничений; принадлежности и непринадлежности узлов; принадлежности и непринадлежности подов; предельных сроков.
Планировщик Kubernetes ищет pod-оболочки, которые еще нигде не выполняются, находит для них подходящие узлы и приказывает утилите kubelet запустить эти pod-оболочки на найденных узлах
Такие ресурсы, как развертывания, представлены записями во внутренней базе данных Kubernetes. Внешне эти ресурсы могут иметь форму текстовых файлов 
(известных как манифесты) в формате YAML. Манифест — это объявление желаемого состояния ресурса.
kubectl — основной инструмент для взаимодействия с Kubernetes, который позволяет применять манифесты, запрашивать данные о ресурсах, вносить изменения, удалять ресурсы и многое другое

Прокси-сервер Kube отвечает за низкоуровневые сетевые функции на каждом узле.  Он предоставляет локальный доступ к сервисам Kubernetes и может выполнять перенаправление в TCP и UDP. Смотрить в Kube-API. 
Для поиска IP-адресов в кластере используются переменные среды или DNS. Также на каждой ноде запускается простой proxy-балансировщик. Этот сервис запускается на каждой ноде и настраивается в Kubernetes API. 
Kube-Proxy может выполнять простейшее перенаправление потоков TCP и UDP (round robin) между набором бэкендов. Управляет сетевыми правилами на нодах. Фактически реализует Service (iptables или ipvs).
Абстракция Service - это набор правил iptables или ipvs. В манифесте Service, описывается то как он будет работать и по каким портам. В манифесте, в соответствующих полях указываются значения, которые после применения в кластере
преобразовываются в правила iptables или ipvs. После создания Service и ему присваевается ip-адрес, через grep по имени в таблице nat на хосте можно посмотреть что представляет из себя Service, как правило iptables или ipvs.
Но сам Service пакеты по протоколу icmp-ping не принимает если это правила iptables. Эти правила при создании Service генерирует Kube-Proxy на каждой ноде. После они начинают распределять трафик на поды. Трафик приходит на ip-адрес
Service и по условиям правила внутри Service трафик идет в указанную в правиле цепочку а за этой цепочкой могут находиться дополнительные цепочки которые могут принимают оставшийся трафик.        

Kubelet — это представитель Kubernetes на каждом узле, единственный компонент который работает не в docker, отдает команды docker daemon, создает поды. 
Он отвечает за взаимодействие с ведущими компонентами и управляет запущенными подами. 
Его обязанности: загрузка конфиденциальных данных пода с API-сервера; подключение томов; запуск контейнера пода (через CRI или rkt); 
уведомление о состоянии узла и каждого экземпляра пода; проверка работоспособности контейнеров.

AppArmor - это модуль безопасности ядра Linux, который дополняет стандартные права пользователей и групп Linux для ограничения программ ограниченным набором ресурсов. 
AppArmor можно настроить для любого приложения, чтобы уменьшить его потенциальную поверхность для атаки и обеспечить более глубокую защиту. 
Он настраивается с помощью профилей, настроенных для внесения в белый список доступа, необходимого для конкретной программы или контейнера, таких как возможности Linux, 
доступ к сети, права доступа к файлам и т. Д. 
Каждый профиль может быть запущен либо в принудительном режиме, который блокирует доступ к запрещенным ресурсам, либо в режиме подачи жалоб. 
Режим, который сообщает только о нарушениях.
AppArmor может помочь вам выполнить более безопасное развертывание, ограничив, что разрешено делать контейнерам, и / или обеспечив лучший аудит с помощью системных журналов. 
Тем не менее, важно помнить, что AppArmor - это не серебряная пуля, и он может лишь очень много сделать для защиты от эксплойтов в коде вашего приложения. 
Важно обеспечить хорошие ограничительные профили, а также укрепить ваши приложения и кластер с других точек зрения.

Каждый рабочий узел в кластере Kubernetes отвечает за следующие компоненты.
kubelet отвечает за управление средой выполнения контейнера, в которой запускаются рабочие задания, запланированные для узла, а также за мониторинг их состояния.
kube-proxy занимается сетевой магией, которая распределяет запросы между pod-оболочками на разных узлах, а также между pod-оболочками и Интернетом.
Среда выполнения контейнеров запускает и останавливает контейнеры, а также отвечает за их взаимодействие. 
Обычно это Docker, хотя Kubernetes поддерживает и другие среды выполнения контейнеров, такие как rkt и CRI-O.
Кроме как выполнением разных программных компонентов, ведущие и рабочие узлы принципиально ничем не отличаются. 
Хотя ведущий узел обычно не выполняет пользовательские рабочие задания, исключение составляют очень маленькие кластеры (такие как Docker Desktop или Minikube).

Понятие пространства имен
Пространства имен предназначены для использования в средах с большим количеством пользователей, распределенных по нескольким командам или проектам. 
Для кластеров с несколькими или десятками пользователей вам вообще не нужно создавать или думать о пространствах имен. 
Начните использовать пространства имен, когда вам понадобятся предоставляемые ими функции.
Пространства имен предоставляют область для имен. Имена ресурсов должны быть уникальными в пределах пространства имен, но не в разных пространствах имен. 
Пространства имен не могут быть вложены друг в друга, и каждый ресурс Kubernetes может находиться только в одном пространстве имен.
Пространства имен - это способ разделить ресурсы кластера между несколькими пользователями (через квоту ресурсов ).
Нет необходимости использовать несколько пространств имен для разделения немного разных ресурсов, например разных версий одного и того же программного обеспечения: 
используйте этикетки чтобы различать ресурсы в одном пространстве имен. 

Когда создается pod, ему назначается один из классов качества обслуживания (Quality of Service, QoS):
гарантированный (guaranteed); взрывной (burstable); негарантированный (best effort).
Pod назначается гарантированный QoS-класс, если его запросы и лимиты совпадают (как для процессора, так и для памяти). 
Взрывной QoS-класс назначается, когда лимиты превышают запросы; это значит, что, помимо гарантированных ресурсов, может использовать дополнительные ресурсы, определяемые его лимитом. 
Негарантированный QoS-класс применяется, когда у контейнеров pod нет ни запросов, ни лимитов.

Понятия платформы Docker
Docker – это платформа для упаковки, распространения и выполнения приложений. Она позволяет упаковывать приложение вместе со всей его средой. 
Это могут быть либо несколько библиотек, которые требуются приложению, либо даже все файлы, которые обычно доступны в файловой системе установленной операционной системы. Так же, Контейнер состоит из операционной системы, 
пользовательских файлов и метаданных, каждый контейнер создается из образа. Этот образ говорит docker-у, что находится в контейнере, какой процесс запустить, когда запускается контейнер и другие конфигурационные данные.
Docker позволяет переносить этот пакет в центральный репозиторий, из которого он затем может быть перенесен на любой компьютер, на котором работает Docker, и выполнен там
Пытаясь развернуть ваши приложения или настроить определенный стек в среде операционной системы на взаимодействие друг с другом, можете столкнуться со следующими проблемами...
Совместимость с базовой ОС и ее компонентов таких как база данный, вебсервер и прочие элементы стека для поддержки приложения или общей работы всего стека технологий так как компоненты
могут быть разнородными, проблемы совместимости библиотек и зависимостей с ОС. Докер может запускать каждый компонент отдельно друг от друга со своими библиотеками и зависимотями
от одной виртуальной машины и ОС. Какждая служба со своими зависимостями в отдельных контейнерах. Контейнер - полностью изолированные среды со своими сетевыми интерфейсами,
средствами монтирования, процессами и службами, файловой системой. Похоже на виртуальную машину, но все они используют одно и тоже ядро ОС. Так же докер имеет меньшую изоляцию
между контейнерами так как используют общие ресурсы, такие как ядро ОС. При создании Docker-контейнера и приложения в нем, над файловой системой образа создается слой файловой системы, 
который впоследствии можно будет изменить - контейнер слой. И все измения которые будет делать приложение - оно будет делать внутри этого слоя файловой системы контейнера, а в самом образе 
делать изменения не может. Docker образ доступен только для чтения. При остановке и перезапуске работы контейнера все данные остаются внутри и не исчезают. Но при удалении все данные и слой файловой сиситемы удаляются 
безвозвратно если раннее не смонтировали каталог узла внуть контейнера для сохранения всех данных на узле. Если нужно произвести некоторые изменения, верхний слой файловой системы, 
доступен для редактирования.

Архитектура Docker
Docker использует архитектуру клиент-сервер. Docker клиент общается с демоном Docker, который берет на себя тяжесть создания, запуска, распределения ваших контейнеров. Оба, клиент и сервер могут работать на одной системе, 
вы можете подключить клиент к удаленному демону docker. Клиент и сервер общаются через сокет или через RESTful API.

Как работает образ
Образ — это read-only шаблон, из которого создается контейнер. Каждый образ состоит из набора уровней. Docker использует union file system для сочетания этих уровней в один образ
Union file system позволяет файлам и директориями из разных файловых систем прозрачно накладываться, создавая когерентную файловую систему. Одна из причин, по которой docker легковесен это использование таких уровней 
Когда вы изменяете образ, например, обновляете приложение, создается новый уровень. Так, без замены всего образа или его пересборки, как вам возможно придётся сделать с виртуальной машиной, только уровень добавляется или обновляется
И вам не нужно раздавать весь новый образ, раздается только обновление, что позволяет распространять образы проще и быстрее. В основе каждого образа находится базовый образ. Например, базовый образ Ubuntu, или fedora и т.д. 
Так же вы можете использовать образы как базу для создания новых образов. Например, если у вас есть образ apache, вы можете использовать его как базовый образ для ваших веб-приложений.

Union File Sysem или UnionFS — это файловая система, которая работает создавая уровни, делая ее очень легковесной и быстрой. Docker использует UnionFS для создания блоков, из которых строится контейнер. 
Docker может использовать несколько вариантов UnionFS включая: AUFS, btrfs, vfs и DeviceMapper.

Docker-клиент — основной способ взаимодействия с Docker’ом. Главный интерфейс к Docker. Она получает команды от пользователя и взаимодействует с docker-демоном. 
При использовании Docker Command Line Interface (CLI) просто вводим в терминал нужную команду,  которая обычно начинается со слова docker. Затем Docker-клиент использует Docker API для отправки команды на Docker Daemon. 
Docker-движок — клиент-серверное приложение. Docker Daemon — Docker-сервер, отвечающий на Docker API запросы. Он управляет образами, контейнерами, Docker-сетями и тома Docker. Демон за пускается на хост-машине. 
Пользователь не взаимодействует с сервером на прямую, а использует для этого клиент.

Docker Engine - это де-факто среда выполнения контейнеров в отрасли, которая работает в различных операционных системах Linux
Docker Engine позволяет контейнерным приложениям работать в любом месте и согласованно в любой инфраструктуре, 
решая «ад зависимостей» для разработчиков и операционных групп и устраняя «это работает на моем ноутбуке!»

PID NS
1) Процессы в PID ns видят только процессы в том же ns - это значит что если в контейнере запускаем какой-то первый процесс то он виден под pid-1 и далее по росту процессов 
меняется номер pid, но в хостовой системе он виден под pid хостовой системы
2) У каждого PID ns своя нумерация начинается с 1 - в каждом ns есть свой родительский процесс pid-1 который порождает другие процессы подобно системе инициализации, на пример systemd.
3) Когда завершается PID 1 весь ns перестает существовать - pid-1 является родительским процессом для других дочерних процессов. Например в linux системах без pid-1 система не живет.
Значит, без родительского процесса не может существовать дочерних процессов которые он порождает.

NET NS
1) В рамках своего net ns процессы получают свое отдельное сетевое окружние - в этот список входят сетевые интерфейсы и в том числе lo, таблица маршрутизации, правила iptables и сокеты.
Сетевые пространства имен - обеспечивают изоляцию системных ресурсов связанные с сетью: сетевые устройства, IPv4 и IPv6 стеки протоколов, таблицы IP-маршрутизации, 
правила брандмауэра, / proc / net каталог (который является символической ссылкой на / proc / PID / net), Каталог / sys / class / net, различные файлы в / proc / sys / net, порт числа (розетки) и так далее. 
Кроме того, сетевые пространства имен изолировать пространство имен абстрактного сокета домена UNIX Физическое сетевое устройство может находиться только в одной сети. пространство имен. 
Когда пространство имен сети освобождено (т. Е. Когда последний процесс в пространстве имен завершается), его физическая сеть устройства возвращаются в исходное сетевое пространство имен (не в родитель процесса). 
Пара устройств виртуальной сети (veth (4)) обеспечивает конвейерную абстракция, которую можно использовать для создания туннелей между сетью пространства имен, и может использоваться для создания моста к физическому сетевое устройство 
в другом пространстве имен. Когда пространство имен освобождается, находящиеся в нем устройства veth (4) уничтожены. Для использования сетевых пространств имен требуется настроенное ядро. с опцией CONFIG_NET_NS.
2) Veth-устройства представляют собой виртуальные устройства Ethernet. Они могут действовать как туннели между сетевыми пространствами имен для создания моста к физическому сетевому 
устройству в другом пространстве имен, но также может использоваться как автономные сетевые устройства. Veth устройства всегда создаются во взаимосвязанных парах.
Пакеты, передаваемые на одно устройство в паре, немедленно будут переданы на другое устройство. Когда одно из устройств не работает, состояние связи пары не работает. 
Пары veth устройств полезны для объединения сетевых средств ядра. Особенно интересный вариант использования - поместить один конец пары veth в одно сетевое пространство имен, 
а другой конец - в другом сетевом пространстве имен, таким образом разрешая связь между сетевыми пространствами имен. Можно создать net ns и внутри него создать сетевой интерфейс и потом
соединить его с корневым net ns виртуальной парой, это можно сказать кабель с двумя концами. Один конец в один net ns а другой чаще всего в bridge в хостовом net ns так можно соединить 
два net ns. Один интерфейс в net ns контейнера а другой интерфейс в хостовом net ns. Для создания такой пары есть утилита ip которая создаст два таких интерфейса на хосте.

MNT NS
1) Процесс может иметь свою root fs - когда создается контейнер то внутри можно увидеть отдельную файловую сиситему которая была поднята из образа - это как раз и есть mnt ns
2) Процесс может иметь свои приватные маунты - помимо того что процесс может делать свой mnt ns, то есть у него там отдельная root fs, так еще и приватные mnt ns свои 
как /tmp, /sys и /proc данные папки частично монтируются из файловой системы контейнера и частично из хоста - это значит что некоторые опции sysctl можно тюнинговать, но далеко не все. 

USER NS
1) В реальности мы получаем пользователя root в контейнере который мапиться в хостовую ns. То есть root в контейнере - это root на хосте, но они изолированы друг от друга и это хорошо, 
но в docker достаточно багов которые позволяют из контейнера вывалиться на хост. Но docker позволяет мапить GID/UID - Можно смапить UID 0 - root в нерутовый UID в хостовом NS.
Для чего это - Лучший способ предотвратить атаки с повышением привилегий из контейнера это настроить приложения контейнера для работы от имени непривилегированных пользователей. 
Для контейнеров, процессы которых должны запускаться от имени root пользователя в контейнере, вы можете переназначить этого пользователя менее привилегированному пользователю на хосте
Сопоставленному пользователю назначается диапазон UID, которые функционируют в пространстве имен как обычные UID от 0 до 65536, но не имеют никаких привилегий на самом хост-компьютере.
Но к сожалению этот подход не всегда есть возможность осуществить так как есть образы которые разворачивают приложения с условием что они будут работать только под root.

Cgroups
1) Docker также использует технологию cgroups или контрольные группы. Ключ к работе приложения в изоляции, предоставление приложению только тех ресурсов, которые вы хотите предоставить. 
Это гарантирует, что контейнеры будут хорошими соседями. Контрольные группы позволяют разделять доступные ресурсы железа и если необходимо, устанавливать пределы и ограничения. Ограничить возможное количество памяти контейнеру.