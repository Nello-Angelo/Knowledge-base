kubectl label nodes/namespace имя ноды/пространства ключь=значение метки (создаем метки для узла чтобы можно было через нее указать что хотим разместить pod именно на узле с такой меткой)
kubectl get nodes --show-labels (что бы посмотреть метки на нодах, но можно и через describe)
kubectl describe pod/replicaset/deployment/node/... имя того, что хочешь посмотреть (после одного из четырех пишем имя, описание которого хотим посмотреть, покажет полное описание)
kubectl get pod/replicaset/deployment/node/... имя того, что хочешь посмотреть -o wide (покажет информацию о запущенных контейнерах с дополнительными полями ip и node) kubectl get pods -w (что бы посмотреть процессы с подами на ходу)
kubectl delete pod/deployment/replicaset/quota/all --all -n пространство имен (удалить все из указанного, если не указывать all и --all, то нужно указать отдельное имя того, что удаляем)
kubectl create -f имя ямл файла (создаст pod из ямл файла с конфигурацией) kubectl apply -f имя ямл файла (создаст обстракцию кубернетеса из ямл файла с конфигурацией или применить изменения из ямл файла на живую не удаляя) 
kubectl create configmap firebase-conf --from-file=firebase-notificator.json (создание конфигмапа из другого файла-конфигурации, так можно делать с любыми типами файлов)
kubectl replace -f имя ямл файла (эта команда применяется тогда, когда ты внутри файла изменил кол-во реплик и мы перечитываем ямл файл)
kubectl scale --replicas=кол-во -f имя файла (этой командой мы изменяем кол-во реплик в ямл файле не заходя в него)
kubectl get replicaset (посмотреть список созданных репликасет) kubectl get replicasets (посмотреть старый и новый набор реплик после обновления деплоймента) kubectl get deployments (чтобы посмотреть список созданных деплойментов)
kubectl delete replicaset/deployment/pod имя одного из них в ямл файле (удалит экземпляр одного из них, которые созданы)
kubectl get all (покажет все репликасет, деплойменты и поды) kubectl get secrets (покажет все секреты в кластере)
kubectl rollout status deployment/имя деплоймента (чтобы посмотреть состояние выкатки обновлений на ходу)
kubectl rollout restart deployment/имя деплоймента (передеплоить текущую версию)
kubectl rollout history deployment/имя деплоймента (для просмотра списка ревизий и истории изменений)
kubectl rollout undo deployment/имя деплоймента (что бы откатиться назад к старой версии, если что-то пошло не так в новой)
kubectl edit deploy имя деплоймента --record (откроет манифест с большим количеством параметров, которые не указывались ямл файле, из которого создан деплоймент, для глобальных изменений)
kubectl exec -it имя пода bash (зайдем в под с командной оболочкой bash)
kubectl get nodes (покажет ноды) kubectl top nodes (покажет ноды в ресурсами потребления)
kubectl get rolesbinding/role -n пространство (покажет имеющиеся роли или связки для пространства) kubectl get clusterrolebinding/clusterrole (выведет все кластерные связки в кластере или роли)
kubectl get quota -n название пространства имен (выведет все квоты в указанном пространстве)(через describe с указанием имени квоты и пространства можно посмотреть насколько занята квота)
kubectl get serviceaccount (покажет все сервис аккаунты в кластере)  kubectl get namespace (покажет все пространства имен в кластере)
kubectl logs имя пода -с имя контейнера как в манифесте (покажет логи всего контейнера) kubectl logs -f имя пода (покажет логи всего пода)
kubectl logs --tail=10 имя пода (выведет указанное кол-во строк лога с конца)  kubectl logs имя пода (покажет логи пода)
kubectl api-versions (покажет версии api в кластере) kubectl api-resources (покажет api ресурсов в кластере)
docker ps (смотрим список запущенных контейнеров, с ключом -q — «тихий» режим, в котором команда выводит только id контейнеров, -a — показывает все контейнеры, а не только запущенные)
docker ps --filter status=exited --all -q (выводит контейнеры статус которых "вышедший" по id контейнера, доступные значения created, restarting, running, removing, paused, exited, dead) 
docker rm $(docker ps --filter status=exited --all -q) (удаляет вывод контейнеров статус которых "вышедший" по id контейнера)
docker pull ubuntu:14.04 (загрузка образа) docker images (покажет список всех образов, хранящихся на машине) 
docker inspect имя образа (Docker inspect отображает низкоуровневую информацию о конкретном объекте Docker) docker build . (собрать dockerfile в текущей директории)
docker image ls -q | xargs -I {} docker image rm -f {} (удалить все имеджи)


ПРИМЕР КОНФИГУРАЦИИ Pod С ПЕРЕМЕННЫМИ СРЕДЫ 
apiVersion: v1 (версия api зависит от типа службы)
kind: Pod (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-nginx (имя пода, оно может быть, какое захочешь)
   labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
      app: myapp (имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  containers: (под ним создаем описание с заполнением полей для контейнеров)
    - name: container-nginx (имя контейнера, какое захочешь)
      image: redis (образ, который будет скачан из репозиториев и установлен)
      imagePullPolicy: Always(Always-Образ скачивается при каждом запуске,Never-будет использоваться образ,который раннее скачан,IfNotPresent-загрузится и будет использоваться при запуске)
      ports: (порт приложения)
        - containerPort: 8080 (указываем порт)
          protocol: TCP (протокол по которому будут передаваться данные)
      env: (пример создания перененных окружения, ниже указываем переменные по типу - имя переменной и ее значение)
        - name: SERVICE_PORT (имя перменной)
          value: "8080"  (значение)
        - name: SERVICE_IP (имя перменной)
          value: "172.17.0.1" (значение)
        - name: PROTOCOL  (имя перменной)
          value: "https" (значение)
      envFrom: (ПЕРВЫЙ СПОСОБ ОПРЕДЕЛИТЬ ВСЕ ПЕРЕМЕННЫЕ И ЗНАЧЕНИЯ ИЗ СЕКРЕТА И КОНФИГМАПА КАК ПЕРЕМЕННЫЕ СРЕДЫ РАЗОМ)(ЕСЛИ КАК ПЕРЕМЕННЫЕ СРЕДЫ ТО ИЗМЕНЕНИЯ В ЯМЛ КОНФИГМАПА И СЕКРЕТА ПРИМЕНЯТСЯ КОНТЕЙНЕРЕ ТОЛЬКО ПОСЛЕ ПЕРЕДЕПЛОЯ)
        - secretRef: (указаваем тип переменных и модуля) 
            name: my-secret (имя секрета как в манифесте)
        - configMapRef: (указаваем тип переменных и модуля)
            name: common-env (имя конфмапа как в манифесте)
      env: (ПРИМЕР ТОГО, КАК ОПРЕДЕЛИТЬ ЧЕРЕЗ ПЕРЕМЕННУЮ СРЕДЫ ЗНАЧЕНИЯ ВЗЯТЫЕ ИЗ СЕКРЕТА ВНУТРЬ КОНТЕЙНЕРА КАЖДЫЙ ПАРАМЕТР ОТДЕЛЬНО)
       - name: SECRET_USERNAME (имя перменной какое захочешь)
           valueFrom: (ниже определяем откуда берем данные)
             secretKeyRef: (то откуда берем) (значения из configmap определяются также только название этого поля будет configMapKeyRef)
                key: username (имя ключа в секрете значение которого положим в переменную) 
                name: my-secret (имя секрета как в манифесте)
       - name: SECRET_RASSWORD (имя перменной какое захочешь)
           valueFrom: (ниже определяем откуда берем данные)
             secretKeyRef: (то откуда берем) (значения из configmap определяются также только название этого поля будет configMapKeyRef)
               name: my-secret (имя секрета как в манифесте)
               key: password  (имя ключа в секрете значение которого положим в переменную)      
      volumeMounts: (ВТОРОЙ СПОСОБ КАК ОПРЕДЕЛИТЬ КОНФИГМАП И СЕКРЕТ ВВИДЕ ТОМА НЕ УКАЗАВАЯ ПЕРЕМЕННЫХ СРЕДЫ ВООБЩЕ)(ОПРЕДЕЛЕНИЕ КОНФИГМАПА ИЛИ СЕКРЕТА ВВИДЕ ТОМА ДАСТ ВОЗМОЖНОСТЬ МЕНЯТЬ ЗНАЧЕНИЯ НА ЛЕТУ НЕ ПЕРЕДЕПЛОВЫВАЯ КОНТЕЙНЕР)
        - mountPath: /etc/secret-redis (путь монтирования, если такой папки нет то она создастся автоматически, в этой папке найдем файлы у которые будут называться как переменные из ямл файла)
          name: secret-redis (имя тома который монтирует в контейнер)
          readOnly: true (политика взаимодействия)
        - mountPath: /etc/configs (путь монтирования, если такой папки нет то она создастся автоматически, в этой папке найдем файлы у которые будут называться как переменные из ямл файла)
          name: envs (имя тома который монтирует в контейнер)
  volumes:(определяем том который смонтируем в контейнер, необязательно определять здесь, можно определить и за полем containers на одном уровне, иногда то что определено ниже поля containers может думать что оно является частью тома)
    - name: secret-redis (создаем имя для тома который смонитуем, относится к второму способу определения секрета)
      secret: (определение секрета)
        secretName: my-secret (имя секрета как в манифесте)
    - name: envs (создаем имя для тома который смонитуем, относится к второму способу определения конфигмапа)
      configMap: (определение конфигмапа)
        name: common-env (имя конфигмапа как в манифесте)


ПРИМЕР КОНФИГУРАЦИИ Deployment 
apiVersion: apps/v1 (версия api зависит от типа службы)
kind: Deployment (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
     name: my-nginx (имя пода, оно может быть, какое захочешь)
     labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
         app: myapp (это метка, имя метки и ее значение может быть любым)
spec: (это поле называется спецификация)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
       metadata: (под ним заполняем данные о службе)
          labels: (под ним создаем специальные метки для взаимодействия внутри кубернетис)
             app: myapp (имя метки и ее значение может быть любым)
       spec: (это поле называется спецификация)
           affinity: (афинити создаются для привязкок подов на уровне узла)(ЭТО ТИП ВЗАИМОДЕЙСТВИЯ ТОЛЬКО НА УРОВНЕ УЗЛОВ) (ВЫБИРАТЬ ТОЛЬКО ОДИН ИЗ ПРИВЕДЕННЫХ ВАРИАНТОВ affinity)
             nodeAffinity: (для планирования на конкретном узле)(для распределения подов по конкретным узлам сначала нужно создать для этих узлов специальные метки которые потом укажем)
               requiredDuringSchedulingIgnoredDuringExecution: (ниже описываем что эта affinity обязательно должно быть выполнена при процедуре распределения)
                  nodeSelectorTerms: (под ним можно указать ряд ключей со значениями хостов на которых можно сделать размещение)
                  - matchExpressions: (совподающие выражения, ниже указываем параметры метки на которую ссылаемся)
                    - key: disktype (название ключа метки ноды на которую нужно ссылаться) (метка для узла должа быть создана предварительно командой из списка команд)
                      operator: In
                      values: (ниже указываем значение ключа ноды)
                       - ssd (его значение)
           affinity: (афинити создаются для привязкок подов на уровне узла) (ЭТО ТИПЫ ВЗАИМОДЕЙСТВИЯ НА УРОВНЕ УЗЛОВ И ПОДОВ) 
             podAntiAffinity: (планировщик не будет размещать реплики на одном узле, этот тип распределят на хосты как DaemonSet)(ИЗ ДВУХ ПРЕДСТАВЛЕННЫХ ТИПОВ НИЖЕ, МОЖНО УКАЗАТЬ ТОЛЬКО podAntiAffinity ИЛИ ДВА В ОДНОМ МАНИФЕСТЕ)
                requiredDuringSchedulingIgnoredDuringExecution: (ниже описываем что эта affinity обязательно должно быть выполнена при процедуре распределения)
                  - labelSelector: (метки селектора)
                      matchExpressions: (совподающие выражения, ниже указываем параметры метки на которую ссылаемся)
                      - key: app (название ключа метки пода на которую нужно ссылаться из шаблона пода в манифесте в котором создаем афинити)
                        operator: In 
                        values: (ниже указываем значение ключа из шаблона пода в котором создаем афинити)
                         - store (его значение, в podAntiAffinity пара ключ=значение берутся только из того манифеста в котором создается podAntiAffinity)
                    topologyKey: "kubernetes.io/hostname" (этот ключ означает что не нужно создавать больше одно экземпляра такого pod на каждом хосте, если описанный pod уже имеется на хосте то еще один такой же не будет создан)
             podAffinity: (нужен для того чтобы расположить указанный pod рядом с другими pod, имеющим описанную метку селектора) (ЭТО affinity МОЖЕТ ИДТИ ТОЛЬКО КАК ДОПОЛНЕНИЕ, ЕСЛИ ЕСТЬ ТАКАЯ НУЖДА)
                requiredDuringSchedulingIgnoredDuringExecution: (ниже описываем что эта affinity обязательно должно быть выполнена при процедуре распределения)
                  - labelSelector: (метки селектора)
                      matchExpressions: (совподающие выражения, ниже указываем параметры метки на которую ссылаемся)
                      - key: proga (название ключа метки пода на которую нужно ссылаться из шаблона пода в манифесте на других хостах, то есть расположить рядом с ними)
                        operator: In 
                        values: (ниже указываем значение ключа из шаблона пода на других хостах, то есть расположить рядом с ними)
                         - java (его значение, в podAffinity пара ключ=значение указываются только из манифеста которые развернуты на других хостах, не на мастере)
                    topologyKey: "kubernetes.io/hostname" (этот ключ означает что не нужно создавать больше одно экземпляра такого pod на каждом хосте, если описанный pod уже имеется на хосте то еще один такой же не будет создан)
           nodeName: kube-01 (указываем что хотим создать pod на конкретном хосте по имени)
           nodeSelector:(селектор нод, альтернативный способ для размещения подов, можно и через nodeAffinity)(метка для узла должа быть создана предварительно командой из списка команд)
              beta.kubernetes.io/os: linux (эта метка означает, что нужно запускать поды только на хостах с Linux ОС)
              disk: ssd (эта метка означает, что нужно запускать поды только на хостах, у которых метка SSD)(метки, которые относятся хостам могуть быть любые)
           securityContext: (параметры безопасности)
              runAsNonRoot: true (не запускать от имени root)
              runAsUser: 1000 (запустить процесс под таким UID, можешь указать любой)
              runAsGroup: 1000 (запустить процесс под таким GID, можешь указать любой)
           containers: (под ним создаем описание с заполнением полей для контейнеров)
              - name: container-nginx (имя контейнера, какое захочешь)
                image: nginx (образ, который будет скачан из репозиториев и установлен)
                imagePullPolicy: Always (политика вытягивания образов, варианты опций Always - всегда вытягивать, Never - никогда не вытягивать, IfNotPresent - если не присутствует)
                securityContext: (параметры безопасности)
                  allowPrivilegeEscalation: false (политика повышенных привелегий)
                ports: (порт приложения)
                 - containerPort: 80 (указываем порт)
                   protocol: TCP (протокол по которому будут передаваться данные)
                volumeMounts: (описываем том)
                 - name: empty-volume (имя тома должно совпадать с разделом volumes:)
                   mountPath: /empty (точка монтирования, если ее не существует то кубернетес создаст ее)
                resources: (ниже описываем то, сколько ресурсов нужно выделить для пода, и запомни, если ты запросил больше, чем есть на сервере, то деплой будет всегда в ожидании)
                  requests: (запрос на выделение ресурсов, под ним указываем какие параметры и в каком обьеме выделить)
                     memory: 3000Mi  (оперативная память)
                     cpu: 1000m  (процессор)
                  limits: (предел обьема ресурсов, который можно выделить, под ним указываем какие параметры и до какого предела ограничить)
                     memory: 5000Mi (оперативная память)
                     cpu: 5000m (процессор)
                startupProbe: (проверяет запустится ли приложение в принципе)
                 failureThreshold: 30 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 timeoutSeconds: 10 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 30 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
                readinessProbe: (проверяет готово ли приложение принимать трафик, при неудаче убирается из балансировки, исполняется постоянно)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна начинаться каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 10 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 60 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
                livenessProbe: (контроль за состоянием приложения во время его жизни, исполняется постоянно, при наличии ошибок будет перезапущен)
                 failureThreshold: 3 (количество допустимых провалов пробы, без удаления из балансировки)
                 httpGet: (отправляет запрос для проверки доступности пода, ниже указываем, куда делать запрос)
                  path: / (путь, куда пойдет запрос, на каждом проде может быть разный)
                  port: 80 (номер порта для доступа к контейнеру. Номер должен быть в диапазоне от 1 до 65535)
                 periodSeconds: 10 (время через которое должна проходить каждая проверка в секундах, через 10 секунд, через 60 секунд...)
                 successThreshold: 1 (пишем кол-во удачных проверок для сброса счетчика "неудачных попыток", то есть, достаточно одной удачной пробы, чтобы считать, что приложение рабочее)
                 timeoutSeconds: 1 (время задержки перед пробой в секундах)
                 initialDelaySeconds: 60 (время в течении которого нельза применять пробу после ее начала, то есть, проба запустится через столько секунд, через сколько мы укажем)
           restartPolicy: Always (политика перезапуска если возникают проблемы в роботе)
           priorityClassName: my-priority (указывается, чтобы определить приоритет создания подов для планировщика)
           serviceAccountName: maylo_013(в продакшене некотрые аккаунты не могут создавать службы из-за ограничений, чтобы создать то что вы хотите нужно указать тот у которого есть права)
           initContainers: (запускаются при инициализации pod и должен описываться до основных контейнеров, подготавливают окружение для работы, выполнение миграций, проверки, дождаться сервис и т.д.)
            - name: init-myservice(имя контейнера, какое захочешь)(работают как обычные контейнеры,всегда выполняются до завершения, должен успешно завершиться, чтобы запустился следующий)
              image: busybox (образ, который будет скачан из репозиториев и установлен)
              command: ['sh', '-c', 'touch /empty/init-file'] (команды, которые будут выполняются перед запуском основного контейнера) 
              volumeMounts: (описываем том) (все действия над emptyDir: будут видны и главному контейнеру)
               - name: empty-volume (имя тома должно совпадать с разделом volumes:)
                 mountPath: /empty (точка монтирования, если ее не существует то кубернетес создаст ее)
           volumes: (ниже описываем том) 
            - name: empty-volume (данный тип тома монтируется в pod для всех контейнеров и данный том будет доступен всем контейнерам вместе с его содержимым)
              emptyDir: {} (тип тома "пустой" нужен для размещения кэш файлов, сохранить данные контейнера после сбоя, обмена файлами между несколькими контейнерами в pod, он виден только в pod и контейнерах, но не на хосте)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: myapp (это метка, имя метки и ее значение может быть любым)
  replicas: 3 (кол-во подов, которые нужно создать)
  strategy: (стратегия обновления приложения)
    type: RollingUpdate (обновляет только по заданному кол-во по очереди, нет простоя) и Recreate (сначала удалит старые и только потом создаст новые, имеет простой)
    rollingUpdate: (ниже задаются параметры обновления, они доступны только если тип стоит rollingupdate, у recreate нет параметров, задается только тип)
       maxSurge: 2 (кол-во одновременно создаваемых подов для обновления старых)
       maxUnavailable: 0 (кол-во одновременно удаляемых подов, это число прибавляется к первому параметру и получается общее кол-во обновлений)
  
       
ПРИМЕР КОНФИГУРАЦИИ NodePort (создается для того, чтобы выставить их наружу приложение по протоколу TCP/UDP по указанному порту, Service - вне манифеста представляет собой правило iptables или ipvs)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service  (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
    name: my-service (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
 type: NodePort (указываем тип службы для взаимодействия с внешним миром)
 ports:  (под ним перечисляем порты)
  - port: 80 (порт самой службы - service)
    targetPort: 80 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
    nodePort: 30004 (внешней порт, по которому будет взаимодействие с внешним миром, он развернется на самом узле, чтобы внешние приложения или юзеры могли доcтучаться до контейнера)
    protocol: TCP (протокол по которому будут передаваться данные)
 selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
      app: myapp (имя метки и ее значение может быть любым, но эту метку мы берем из манифеста, который создаст поды, в разделе меток пода)
      

КОНФИГУРАЦИЯ ClasterIP(кластер создают для внутренних сервисов таких как базы данных и т.д. если не указывать тип порта сервиса, то поумолчанию создается кластер, Service - вне манифеста представляет собой правило iptables или ipvs)
apiVersion: v1 (версия api зависит от типа службы)
kind: Service (пишем тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: service (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
  clusterIP: 10.28.90.25 (при необходимости можно вручную прописать желаемый ip-адрес)
  ports: (под ним перечисляем порты)
      port: 80 (порт самой службы - service)
      targetPort: 80 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
      port: 443 (порт самой службы - service)
      targetPort: 8443 (порт самого пода, на котором контейнер приложения слушает внешний мир, номер порта, который нужно указать, указан в поле containerPort в манифесте деплоя)
      protocol: TCP (протокол по которому будут передаваться данные)
  selector: (селектор помогает деплойменту понять, какие поды пренадлежат ему, метка в селекторе и в шаблоне должы быть одинаковые)
      name: app (имя метки и ее значение может быть любым, но эту метку мы берем из манифеста, который создаст поды, в разделе меток пода)
   
   
ПРИМЕР КОНФИГУРАЦИИ HorizontalPodAutoscaler (нужен чтобы автоматически создавать поды в кластере основываясь на метриках потребления ресурсов)
apiVersion: autoscaling/v2beta2 (версия api зависит от типа службы)
kind: HorizontalPodAutoscaler (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
   name: my-auto-scaler  (имя сервиса, оно может быть, какое захочешь)
spec: (это поле называется спецификация)
   scaleTargetRef: (описываем тот деплоймент, который будем мониторить, версия api, kind службы и его name нужно смотреть в ямл файле самой службы)
       apiVersion: apps/v1 (версия api зависит от типа службы)
       kind: Deployment (тип службы, которую мы создаем если превышены параметры описанные ниже)
       name: my-nginx (имя службы, у которой мониторим параметры, смотреть в самом деплойменте)
   minReplicas: 2 (минимальное кол-во реплик соданных если параметры будут превышены)
   maxReplicas: 6 (максимальное кол-во реплик соданных если параметры будут превышены)
   metrics: (ниже указываем метрики и предел параметров исходя из которых будет принято решение создать дополнительные поды для распределения нагрузки)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: cpu (название ресурса, процессор)
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)
    - type: Resource (тип того что мониторим)
      resource: (ниже указываем типы ресурсов)
         name: memory (название ресурса, оперативная память) 
         targetAverageUtilization: 80 (предел обьема ресурсов, при котором будет принято решение создать новый под, предел можно указать любой)  


ПРИМЕР КОНФИГУРАЦИИ Job 
apiVersion: batch/v1 (версия api зависит от типа службы)
kind: Job (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: hello (имя службы)
spec: (это поле называется спецификация)
  ttlSecondsAfterFinished: 50 (указывает, через сколько секунд специальный TimeToLive контроллер должен удалить завершившийся Job вместе с подами и их логами)
  backoffLimit: 1 (количество попыток. Если указать 2, то Job дважды попробует запустить под и остановится)
  activeDeadlineSeconds: 20 (количество секунд, которое отводится всему Job на выполнение)
  completions: 1 (сколько подов должны успешно завершиться, прежде чем вся задача будет считаться сделанной)
  parallelism: 1 (кол-во подов, которые будут запущены паралельно)
  template: (это шаблон, под ним описываем то, какие контейнеры нужно создать)
    spec: (это поле называется спецификация)
      containers: (под ним создаем описание с заполнением полей для контейнеров)
      - name: hello (имя контейнера)
        image: nginx (образ, который будет скачан из репозиториев и установлен)
        args: (под ним указываем команды, которые будут выполнятся в контейнере)
        - /bin/bash (командная оболочка)
        - -c
        - apt upgrade    
      restartPolicy: Never (политика перезапуска контейнера)


ПРИМЕР КОНФИГУРАЦИИ Secret С ОПРЕДЕЛЕНИЕМ ЛОГИНА И ПОРОЛЯ В КОДИРОВКЕ base64
apiVersion: v1 (версия api зависит от типа службы)
kind: Secret (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: my-secret (имя службы)
type: Opaque
data: (под ним указываем данные, которые будут храниться в секрете, ключь и значение могут быть любые и должны всегда быть в кодировке)
  username: YWRtaW4= (логин в кодировке base64)
  password: bWljaGE= (пароль в кодировке base64)


ПРИМЕР КОНФИГУРАЦИИ ConfigMap С ПЕРЕМЕННЫМИ СРЕДЫ
apiVersion: v1 (версия api зависит от типа службы)
kind: ConfigMap (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о службе)
  name: apps (имя службы)
  labels: (это метка, имя метки и ее значение может быть любым)
    app: back (метка)
    label: back (метка)
data: (ниже описываем переменные и значения которые хотим задать для приложения которое после запуска прочитает их и применит их к своей конфигурации, можно определить в манифесте целиком, а не каждую отдельно) 
  Jwt__Authority: http://any/path/site (переменная и значение)
  oidc__IssuerUrl: http://any/path/site (переменная и значение)
  oidc__ClientId: apps (переменная и значение)
  oidc__TokenUrl: http://any/path/site (переменная и значение)


      
      tolerations: (ниже описываем толерантность к размещению подов на хостах)
      - effect: NoSchedule(NoSchedule-запрещает размещать на узле, PreferNoSchedule-размещает на узле если нельзя где-то еще, NoExecute-выселяет с узла уже запущенные pod, NodeCondition-действует для узлов отвечающих заданному условию)
        operator: Exists(указываем что pod должен создаваться на хостах у которых поле taints: NoSchedule, это поле можно найти если сделать describe хоста)(поле effect: в манифесте это значение в поле taints: из вывода describe хоста) 
      terminationGracePeriodSeconds: 500 
      volumes: (перечень томов для монтирования в контейнер) (hostPath лучше использовать, когда к данным должны обращаться агенты или сервисы уровня узла)
      - name: varlog (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath: (нужен для монтирования папок из файловой системы хоста в контейнер для прямого доступа к папкам хоста, приложению, когда нужно предоставить доступ к файловой системе хоста. К логам приложения /var/log и т.д.)
          path: /var/log (путь из файловой системы хоста) (если монтируем папку которую создали сами то нужно задать на нее те же права что и под разделом securityContext: иначе не сможем с ней работать)
          type: Directory (тип тома)
      - name: varlibdockercontainers (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath: (нужен для монтирования папок из файловой системы хоста в контейнер для прямого доступа к папкам хоста, приложению, когда нужно предоставить доступ к файловой системе хоста. К логам приложения /var/log и т.д.)
          path: /var/lib/docker/containers (путь из файловой системы хоста) (если монтируем папку которую создали сами то нужно задать на нее те же права что и под разделом securityContext: иначе не сможем с ней работать)
          type: Directory (тип тома)
      - name: root (имя тома, в котором будет путь из файловой системы хоста, который укажем ниже)
        hostPath: (нужен для монтирования папок из файловой системы хоста в контейнер для прямого доступа к папкам хоста, приложению, когда нужно предоставить доступ к файловой системе хоста. К логам приложения /var/log и т.д.)
           path: / (путь из файловой системы хоста) (если монтируем папку которую создали сами то нужно задать на нее те же права что и под разделом securityContext: иначе не сможем с ней работать)
           type: Directory (тип тома) 



ПРИМЕР КОНФИГУРАЦИИ ПРОСТОГО ServiceAccount
apiVersion: v1 (версия api зависит от типа службы)
kind: ServiceAccount (тип службы, которую мы создаем)
metadata: (под ним заполняем данные о пользователе)
  name: demo-user-demon (имя пользователя)
  namespace: default (пространство имен, в котором будет распологаться пользователь, на твое усмотрение)


ПРИМЕР КОНФИГУРАЦИИ ClusterRole и Role
apiVersion: rbac.authorization.k8s.io/v1 (версия api зависит от типа службы)
kind: ClusterRole (тип роли)
metadata: (под ним заполняем данные о роли)
  name: admin-admin (имя роли)
  namespace: default (пространство имен, в котором был создан обьект, только для обычной роли указываем)
rules: (ниже указывам список того, к чему будет иметь доступ тот аккаунт, к которому будет привязана эта роль)
- apiGroups: ["", "extensions", "apps"] (api-группы модулей)
  resources: ["deployments", "replicasets", "pods", "services", "nodes", "nodes/stats", "namespaces"] (модули, ноды и пространства имен, ноды и пространства указываются только в ClusterRole)
  verbs: ["list", "create", "update", "patch", "delete"] (выражения, которые можно использовать для управления состоянием кластера)

  
ПРИМЕР КОНФИГУРАЦИИ СВЯЗКИ ClusterRoleBinding и RoleBinding
apiVersion: rbac.authorization.k8s.io/v1 (версия api зависит от типа службы)
kind: ClusterRoleBinding (тип связки для роли и сервис аккаунта)
metadata: (под ним заполняем данные о связке)
  name: admin-deploy-admin (имя связки)
  namespace: default (пространство имен, в котором создаем привязку, только для обычной роли указываем)
subjects: (обьекты, к которым будет привязана роль, данные поля обязательны для заполнения)
- kind: ServiceAccount (тип обькта) (если тип User или Group то это (apiGroup: rbac.authorization.k8s.io) поле определяем вместо поля (namespace: default) в разделе subjects:)
  name: demo-user-demon (имя сервис аккаунта)
  namespace: default (пространство имен, в котором был создан ServiceAccount, только для обычной роли указываем)
roleRef: (заполняем данные о роли, которую привяжем к аккаунту)
  apiGroup: rbac.authorization.k8s.io (api-группа, к которой принадлежит роль)
  kind: ClusterRole (тип роли которую привязываем)
  name: admin-admin (имя роли, как в манифесте)


ПРИМЕР КОНФИГУРАЦИИ ResourceQuota (требует указывать в манифесте лимит и реквест не больше и не меньше тех значений, которые в квоте, ресурсы выделяются на все пространство имен в целом)
apiVersion: v1 (версия api зависит от типа службы)
kind: ResourceQuota (тип службы)(нужно следить за ресурсами во время обновлений, потому что прежде чем удалить старые, будут созданы новые, и может не обновиться из-за дележки ресурсов)
metadata: (под ним заполняем данные о службе)
  name: compute-resources (имя службы)
  namespace: default (имя пространства имен, в котором будет создана квота)
spec: (спецификация)
  hard: (ниже указываем жесткие параметры на лимит, реквест и создание модулей)
    requests.cpu: 300m (минимум для выделения процесс)
    requests.memory: 300Mi (минимум для выделения оперативной памяти)
    limits.cpu: 300m (предел для выделения процесса)
    limits.memory: 300Mi (предел для выделения оперативной памяти)
    count/deployments.apps: "4" (кол-во деплоев, которое можно создавать)
    count/pods: "4" (кол-во подов, которое можно создавать)
    count/replicasets.apps: "4" (кол-во репликасетов, которое можно создавать)


ПРИМЕР КОНФИГУРАЦИИ LimitRange 
apiVersion: v1 (версия api)
kind: LimitRange (обычно используют для определения limit по умолчанию, чтобы поды в которых явно не указали лимит и реквест создавались с значениями из полей по умолчанию из этой службы)
metadata: (информация о службе)
   name: my-limitrange (имя службы)
   namespace: default (пространство имен в котором создадим LimitRange)
spec: (спецификация)
  limits: (ниже определяем ограничения)
  - type: Container (явно указанные параметры этого типа распростроняются на каждый модуль и реплику, которой требуются ресурсы в пространстве, а не на все пространство в целом, как в квоте, в одном пространстве - один LimitRange)
    max: (лимит)
      cpu: 2 (процессор)
      memory: 1Gi (оперативная память)
    min: (реквест) (под контейнером задавать не ниже указанных параметров, иначе не создаст ничего, так как не соблюдены условия по ресурсам)
      cpu: 200m (процессор)
      memory: 500Mi  (оперативная память)
  - type: Container (тип того, к чему будут применятся значения по умолчанию, в каждом модуле и в каждой реплике, которым требуются ресурсы в том пространстве, в котором определен LimitRange)
    default: (лимит по умолчанию)
      cpu: 300m (процессор)
      memory: 200Mi (оперативная память)
    defaultRequest: (реквест по умолчанию)
      cpu: 200m (процессор)
      memory: 100Mi (оперативная память)


ПРИМЕР КОНФИГУРАЦИИ PodDisruptionBudget (приложение останется доступным даже во время выселения по причине аппаратных неполадок, недоступностью сети или сбоями в ядре и т.д.)
apiVersion: policy/v1beta1 (версия api)
kind: PodDisruptionBudget (тип службы)
metadata: (информация о службе)
  name: frontend-pdb (имя службы)
spec: (спецификация)
  minAvailable: 5 (всегда должно иметь не менее пяти реплик, Kubernetes может выселить любое количество pod при условии, что пять из них остаются доступными)
 #maxUnavailable: 20%(в любой отдельный момент времени недоступными могут быть не более 20 % pod)(МОЖНО УКАЗАТЬ ТОЛЬКО ОДНО ИЗ ПРИВЕДЕННЫХ ПРАВИЛ СОХРАНЕНИЯ ПОДОВ, ЛИБО min, ЛИБО max)
  selector: (селектор помогает понять, какие поды нужно сохранять, метка в селекторе и в шаблоне должы быть одинаковые)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: frontend (метка)


ПРИМЕР КОНФИГУРАЦИИ PriorityClass
apiVersion: scheduling.k8s.io/v1 (версия api)
kind: PriorityClass (тип службы)
metadata: (информация о службе)
  name: high-priority-nonpreempting (имя службы)
value: 100 (значение, по которому будет определятся приоритет планирования подов, чем выше значение, тем выше приоритет перед другими пода)
preemptionPolicy: Never(Если стоит Never поды этого приорити-класс не будут вытеснять поды, по умолчанию стоит PreemptLowerPriority, позволяет вытеснять модули с более низким приоритетом)
globalDefault: false (если значение поля стоит true, то использовать для подов без указания в них поля priorityClassName, политика будет работать глобально, если false, то указываем поле)
description: "описание - здесь описываем функционал манифеста"



ПРИМЕР КОНФИГУРАЦИИ NetworkPolicy egress (сетевые политики могут разрешать или запрещать входящий или исходящий трафик, он похожи на правила для брендмауера)
apiVersion: networking.k8s.io/v1 (версия api) (в документации можно также посмотреть прмеры с использованием ipBlock чтобы выбрать подсеть, но это уже специфично для конкретной ситуации)
kind: NetworkPolicy (тип службы) (после создания все что нужно трафик нужно пустить через веб-сервер nginx или apache)
metadata: (информация о службе)
  name: default.nginx (имя службы)
  labels: (ниже содаем метку, имя метки и ее значение, он может быть любым)
    netw: network (метка)
  namespace: default (имя пространства имен, в котором будет создана сетевая политика)
spec:  (спецификация)
  podSelector: (под селектором подов указываем метку пода к кторому нужно привязать политику) 
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: nginx (метка из пода в разделе шаблона)
  egress: (тип правила, исходящий трафик)
  - to: (под ним указаваем направление куда нужно отправлять трафик для пода)(ветвлений с to может быть неограниченное кол-во, для одного пода - одно to:, а для другого пода - другое to:)
    - podSelector:(под селектором подов указываем метку пода из которого будут присылать трафик)(если хотим под одним портом и to указать несколько подов создаем каждый как новое правило)
        matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
          app: redis-app (метка из пода в разделе шаблона)
      namespaceSelector: (селектор namespaceSelector ссылается на ту метку которая была создана командой предварительно перед размещение, создавать метки нужно для любого пространства)        
        matchLabels: (селектор matchLabels из namespaceSelector сопостовляет метку указанную в политуке с той что создана для пространства)           
          namespace: default (метка которую создали для пространства имен)    
    ports: (ниже указаваем порты, указываем порт dns 53 чтобы политика разрешала исходящему трафику обращаться к dns, исходящий трафик будет идти через него, хотя можно и другой)
    - protocol: UDP (протокол dns) 
      port: 53 (порт dns) (каждому to можно указать свой порт)
  policyTypes: (указываем тип политики которую применяем)
  - Egress (тип политики)



ПРИМЕР КОНФИГУРАЦИИ NetworkPolicy ingress (сетевые политики могут разрешать или запрещать входящий или исходящий трафик, он похожи на правила для брендмауера)
apiVersion: networking.k8s.io/v1 (версия api) (в документации можно также посмотреть прмеры с использованием ipBlock чтобы выбрать подсеть, но это уже специфично для конкретной ситуации)
kind: NetworkPolicy (тип службы) (после создания все что нужно трафик нужно пустить через веб-сервер nginx или apache)
metadata: (информация о службе)
  name: database.redis (имя службы)
  labels: (ниже содаем метку, имя метки и ее значение, он может быть любым
    netw: network (метка)
  namespace: default (имя пространства имен, в котором будет создана сетевая политика)
spec: (спецификация)
  podSelector: (под селектором подов указываем метку пода к кторому нужно привязать политику)
    matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
      app: redis-app (метка из пода в разделе шаблона)
  ingress: (тип правила, входящего трафик)
  - from:(под ним указаваем откуда нужно принимать трафик для пода)(ветвлений с from может быть неограниченное кол-во, для одного пода - одно from:, а для другого пода - другое from:)
    - namespaceSelector: (селектор namespaceSelector ссылается на ту метку которая была создана командой предварительно перед размещение, создавать метки нужно для любого пространства)
        matchLabels: (селектор matchLabels из namespaceSelector сопостовляет метку указанную в политуке с той что создана для пространства)
          namespace: default (метка которую создали для пространства имен) 
      podSelector:(под селектором подов указываем метку пода из которого будут присылать трафик)(если хотим под одним портом и from указать несколько подов создаем каждый как новое правило)
        matchLabels: (селектор matchLabels сопостовляет свои метки с полем labels из пода в разделе шаблона)
          app: nginx (метка из пода в разделе шаблона)
    ports: (ниже указаваем порты, указываем порт через который будет входить трафик, указываем ssh)
      - port: 22 (порт входящего трафика может быть любым)(каждому from можно указать свой порт)
        protocol: TCP (протокол)
  policyTypes: (указываем тип политики которую применяем)
  - Ingress (тип политики)